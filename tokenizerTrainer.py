
import os

from itertools import product
from tokenizers import Tokenizer, Regex
from tokenizers.models import WordLevel
from tokenizers.normalizers import NFD
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Split, WhitespaceSplit

## Loop which creates and loads the tokenizer
def stackTraceTokenizer(tokens:tuple, events:tuple, vocab_size=2_000, min_freq=3):

    for norm, event in product(tokens, events):
        print(norm, event)

        tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))
        tokenizer.normalizer = NFD()

        if norm == 'white':
            tokenizer.pre_tokenizer = WhitespaceSplit()
        else:
            tokenizer.pre_tokenizer = Split(pattern=Regex("[A-Z]+[a-z0-9]+|[.A-Z]+|[a-z0-9]+"),
                                            behavior='isolated')

        trainer = WordLevelTrainer(vocab_size=vocab_size, min_frequency=min_freq,
                                  special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

        tokenizer.train(["vocab-{}.txt".format(event)], trainer)
        print(f"Trained tokenizer for {norm}:{event}")

        yield tokenizer, event
