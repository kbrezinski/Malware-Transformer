
from itertools import product
from tokenizers import Tokenizer, Regex
from tokenizers.models import WordLevel ##
from tokenizers.normalizers import NFD
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Split, WhitespaceSplit

## Creating the loop creating and loading the tokenizer
for norm, event in product(['white','token'], ['Reg', 'File', 'Thread']):

    tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))
    tokenizer.normalizer = NFD()

    if norm == 'white':
        tokenizer.pre_tokenizer = WhitespaceSplit()
    else:
        tokenizer.pre_tokenizer = Split(pattern=Regex("[A-Z]+[a-z0-9]+|[.A-Z]+|[a-z0-9]+"),
                                        behavior='isolated')

    trainer = WordLevelTrainer(vocab_size=2_000, min_frequency=3,
                              special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

    tokenizer.train([f"vocab-{event}.txt"], trainer)
    tokenizer.model.save('./')
