
import os
import glob
import sys
import math
import torch

from tokenizerTrainer import stackTraceTokenizer
from dataLoader import stackTraceDataloader
from transformer import TorchTransformer
from config import cfg

device = 'cuda' if torch.cuda.is_available() else 'cpu'

class Trainer:

    def __init__(self, dataLoader, cfg_data):
        self.dataloader = dataLoader
        self.cfg_data = cfg_data
        self.net = TorchTransformer ## need inputs
        self.epoch = 0

        self.train_record = {
            'best_recall': 0.,
            'best_prec': 0.,
            'best_f1': 0.,
            'best_model':''}

        self.train_dl, self.val_dl = self.dataloader(*params)

        self.optimizer = torch.optim.Adam(model.parameters(), lr=cfg_data.LR)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1.0, gamma=cfg_data.GAMMA)

    def forward(self):
        for epoch in range(self.epoch, cfg.MAXEPOCHS):
            self.epoch = epoch

            self.train()
            self.validate()

    def train(self):
        self.net.train()
        total_loss = 0.
        total_acc = 0

        for idx, batch in enumerate(self.train_dl):
            data, inp_len, targets = batch

            data = data.to(device)
            targets = targets.to(device)

            if data.size(0) != cfg.MAXLEN:
                src_mask = model.gen_mask(data.size(0)).to(device)

            self.optimizer.zero_grad()
            output, loss = self.net(data, targets, src_mask, inp_len)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.CLIPGRAD)
            optimizer.step()

            total_acc += torch.sum(torch.round(output) == targets)
            total_loss += loss.item()

        print(f"| epoch: {self.epoch:2d} | lr: {self.scheduler.get_last_lr()[0]:02.5f} \
                | loss: {total / idx:5.5f} | acc: {total_acc / len(data)}")

    def validate
